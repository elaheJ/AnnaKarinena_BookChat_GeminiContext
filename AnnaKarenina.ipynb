{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elaheJ/AnnaKarinena_BookChat_GeminiContext/blob/main/AnnaKarenina.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "2p8VrsFXZnPZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q BeautifulSoup\n",
        "!pip install torch\n",
        "!!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b3fdXCl-PsVZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "import datetime\n",
        "import time\n",
        "from google.colab import userdata\n",
        "myGKey = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=myGKey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8V-aqBjqP_Uy"
      },
      "outputs": [],
      "source": [
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gq4Z0DnqPzAu"
      },
      "outputs": [],
      "source": [
        "#By Paul Mooney https://www.kaggle.com/code/paultimothymooney/how-to-upload-large-files-to-gemini-1-5/notebook\n",
        "\n",
        "def upload_to_gemini(path, mime_type=None):\n",
        "  \"\"\"Uploads the given file to Gemini.\n",
        "\n",
        "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
        "  \"\"\"\n",
        "  file = genai.upload_file(path, mime_type=mime_type)\n",
        "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
        "  return file\n",
        "\n",
        "def wait_for_files_active(files):\n",
        "  \"\"\"Waits for the given files to be active.\n",
        "\n",
        "  Some files uploaded to the Gemini API need to be processed before they can be\n",
        "  used as prompt inputs. The status can be seen by querying the file's \"state\"\n",
        "  field.\n",
        "\n",
        "  This implementation uses a simple blocking polling loop. Production code\n",
        "  should probably employ a more sophisticated approach.\n",
        "  \"\"\"\n",
        "  print(\"Waiting for file processing...\")\n",
        "  for name in (file.name for file in files):\n",
        "    file = genai.get_file(name)\n",
        "    while file.state.name == \"PROCESSING\":\n",
        "      print(\".\", end=\"\", flush=True)\n",
        "      time.sleep(10)\n",
        "      file = genai.get_file(name)\n",
        "    if file.state.name != \"ACTIVE\":\n",
        "      raise Exception(f\"File {file.name} failed to process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "E2BLCzK_QKAb",
        "outputId": "74346c36-8934-4573-fb52-1cc227b5c6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file 'AnnaKarenina.txt' as: https://generativelanguage.googleapis.com/v1beta/files/ynr0edxrgmgo\n",
            "Waiting for file processing...\n"
          ]
        }
      ],
      "source": [
        "#By Paul Mooney https://www.kaggle.com/code/paultimothymooney/how-to-upload-large-files-to-gemini-1-5/notebook\n",
        "text='/content/drive/MyDrive/Gutenberg/Books/Tolstoy/AnnaKarenina.txt'\n",
        "files = [\n",
        "  upload_to_gemini(text, mime_type=\"text/plain\"),\n",
        "]\n",
        "\n",
        "wait_for_files_active(files)\n",
        "\n",
        "chat_session = model.start_chat(\n",
        "  history=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[0],\n",
        "      ],\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "6ETAbAVSS3K-",
        "outputId": "7e63c8e5-54ad-4203-9caa-0ac3c96921b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1027: UserWarning: Theme should be a class loaded from gradio.themes\n",
            "  warnings.warn(\"Theme should be a class loaded from gradio.themes\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://025d8ccae32d1c0ae2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://025d8ccae32d1c0ae2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://025d8ccae32d1c0ae2.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import gradio as gr\n",
        "def geminiChatter(message, history):\n",
        "    response = chat_session.send_message(message)\n",
        "    return response.text\n",
        "with gr.Blocks(theme=gr.themes.Soft) as app:\n",
        "      gr.ChatInterface(fn=geminiChatter, type=\"messages\")\n",
        "app.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1oUtrIggCrE2TwljSOWMIfuhZxbTt8CfM",
      "authorship_tag": "ABX9TyNqIrA+CI2nrFfjq5VQgRsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}